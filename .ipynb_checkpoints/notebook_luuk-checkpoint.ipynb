{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "For the sentiment analysis, we tried out several different models and pre-processing pipelines. Especially for dealing with comments or descriptions in the lines, like [laughing] or [to camera], we tried out different methods to see which resulted in the best score for the sentiment analysis.\n",
    "\n",
    "We mainly used sentiment analysis based on pre-trained models, and then tested the accuracy by comparing the predicted sentiment with the sentiments given by us in the annotated sample (of 300 lines).\n",
    "\n",
    "## 1. Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"The_Office_lines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\"id\",\"speaker\", \"line_text\"]\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# deals with descriptions in lines, e.g. [laughs] or [to camera]\n",
    "def deal_with_description(line, mode):\n",
    "    if mode==\"remove\":\n",
    "        # remove text that is between brackets\n",
    "        line = re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"end\":\n",
    "        # move all the text that is in the brackets to the end of the line\n",
    "        line = re.sub(r'\\[.*?\\]', '', line) + \" \" + \", \".join(re.findall(r\"\\[(.*?)\\]\", line))\n",
    "    elif mode==\"start\":\n",
    "        # move all the text that is in the brackets to the start of the line\n",
    "        line = \", \".join(re.findall(r\"\\[(.*?)\\]\", line)) + \" \" + re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"keep\":\n",
    "        # remove all brackets from the line but keep text in place\n",
    "        line = re.sub(r\"[\\([{})\\]]\", '', line)\n",
    "    return line\n",
    "\n",
    "def preprocess_sentiment(df, description_mode):\n",
    "    # deal with descriptions in lines\n",
    "    df[\"line_text\"] = df[\"line_text\"].apply(lambda x: deal_with_description(x, mode=description_mode))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the sentiment analysis first only the the sample labeled by us, and then applied the best performing combination of pipeline and model to the whole dataset.\n",
    "\n",
    "#### Function to extract ids that have been annotated by us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_ids():\n",
    "    df_luuk = pd.read_csv(\"annotated_data/sample_Luuk.csv\")\n",
    "    df_shan = pd.read_csv(\"annotated_data/sample_Shantanu.csv\")\n",
    "    df_elin = pd.read_csv(\"annotated_data/sample_Eline.csv\")\n",
    "\n",
    "    # combine annotations\n",
    "    df_combined = pd.concat([df_luuk, df_shan, df_elin], axis=0)\n",
    "\n",
    "    # filter out only columns that have something in \"Sentiment\" column\n",
    "    df_annotated = df_combined[df_combined[\"Sentiment\"].notna()]\n",
    "    df_annotated.reset_index(drop=True)\n",
    "\n",
    "    return df_annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the accuracy of the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "# translating strings of sentiment to integers\n",
    "trans_dict_roberta = {\n",
    "    \"NEGATIVE\": -1,\n",
    "    \"POSITIVE\": 1\n",
    "}\n",
    "\n",
    "trans_dict_bert = {\n",
    "    \"LABEL_0\": -1,\n",
    "    \"LABEL_1\": 0,\n",
    "    \"LABEL_2\": 1\n",
    "}\n",
    "\n",
    "# extract predicted values from dataframe\n",
    "def extract_ypred(df, source_column, transdict, write=True, target_column=\"temp\"):\n",
    "    df[target_column] = df[source_column].apply(lambda x: transdict[x[0][\"label\"]])\n",
    "    Y_pred = df[target_column].values\n",
    "    if not write:\n",
    "        df = df.drop(columns=[target_column])\n",
    "    return Y_pred\n",
    "\n",
    "def result_score(Y_val, Y_pred, name, binary=False):\n",
    "    # make new list replacing 0 with 1 if binary\n",
    "    if binary:\n",
    "        Y_val = [1 if x==0 else x for x in Y_val]\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(Y_val, Y_pred)\n",
    "    precision = precision_score(Y_val, Y_pred, average=\"macro\")\n",
    "    recall = recall_score(Y_val, Y_pred, average=\"macro\")\n",
    "    f1 = f1_score(Y_val, Y_pred, average=\"macro\")\n",
    "    MSE = mean_squared_error(Y_val, Y_pred)\n",
    "\n",
    "    # print results\n",
    "    print(f\"Analysis with {name}:\\\n",
    "          \\n- - - - - - - - - - \\\n",
    "          \\nAccuracy: {accuracy}\\\n",
    "          \\nPrecision: {precision}\\\n",
    "          \\nRecall: {recall}\\\n",
    "          \\nF1: {f1}\\\n",
    "          \\nMSE: {MSE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find current time\n",
    "import time\n",
    "#supress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fit_sentiment(df_filtered, method, name, progress=True, ret=False):\n",
    "    # set start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # apply sentiment analysis to each line, track progress\n",
    "    df_filtered[name] = \"\"\n",
    "\n",
    "    # apply sentiment analysis to each line and track progress\n",
    "    if progress:\n",
    "        print(f\"Fit sentiment analysis {name}\")\n",
    "    k = len(df_filtered)\n",
    "    i = 0\n",
    "    for iter, row in df_filtered.iterrows():\n",
    "        df_filtered[name][iter] = method(row[\"line_text\"])\n",
    "        if progress:\n",
    "            print(f\"sample {i+1} out of {k}. {round((i+1)/k*100, 2)}%  \", end='\\x1b[1K\\r')\n",
    "        i += 1\n",
    "    if ret:\n",
    "        #drop name column\n",
    "        y_pred = df_filtered[name].values\n",
    "        df_filtered = df_filtered.drop(columns=[name])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import first pre-trained sentiment analysis pipeline\n",
    "from transformers import pipeline\n",
    "sentiment_analysis_roberta = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "sentiment_analysis_bert  = pipeline(\"sentiment-analysis\",model=\"sbcBI/sentiment_analysis_model\")\n",
    "sentiment_analysis_distilbert = pipeline(\"sentiment-analysis\",model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_analysis_bert_uncased = pipeline(\"sentiment-analysis\",model=\"Seethal/sentiment_analysis_generic_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_distilbert(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9952951073646545}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_bert_uncased(\"I hate you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, for now, take only the lines that have been annotated by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only annotated lines\n",
    "df_annotated = annotated_ids()\n",
    "\n",
    "# reset index\n",
    "df_annotated.reset_index(drop=True)\n",
    "\n",
    "# preprocess the lines\n",
    "df_filtered = preprocess_sentiment(df_annotated, description_mode=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis sentiment_analysis_roberta\n",
      "Fit sentiment analysis sentiment_analysis_bert\n",
      "Fit sentiment analysis sentiment_analysis_distilbert\n",
      "Fit sentiment analysis sentiment_analysis_bert_uncased\n",
      "sample 217 out of 217. 100.0%  \u001b[1K\r"
     ]
    }
   ],
   "source": [
    "fit_sentiment(df_filtered, sentiment_analysis_roberta, \"sentiment_analysis_roberta\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert, \"sentiment_analysis_bert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_distilbert, \"sentiment_analysis_distilbert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert_uncased, \"sentiment_analysis_bert_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_345/1238867368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred_roberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ypred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment_analysis_roberta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_dict_roberta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pred_sentiment_label_roberta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ypred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment_analysis_bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_dict_bert\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pred_sentiment_label_bert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred_bert_uncased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ypred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment_analysis_bert_uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_dict_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pred_sentiment_label_bert_uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_345/480119716.py\u001b[0m in \u001b[0;36mextract_ypred\u001b[0;34m(df, source_column, transdict, write, target_column)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# extract predicted values from dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_ypred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_345/480119716.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# extract predicted values from dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_ypred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# get values of Y_val\n",
    "Y_val = df_filtered[\"Sentiment\"].values\n",
    "\n",
    "y_pred_roberta = extract_ypred(df_filtered, \"sentiment_analysis_roberta\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_roberta\")\n",
    "y_pred_bert = extract_ypred(df_filtered, \"sentiment_analysis_bert\", trans_dict_bert,  write=True, target_column=\"pred_sentiment_label_bert\")\n",
    "y_pred_bert_uncased = extract_ypred(df_filtered, \"sentiment_analysis_bert_uncased\", trans_dict_bert, write=True, target_column=\"pred_sentiment_label_bert_uncased\")\n",
    "y_pred_distilbert = extract_ypred(df_filtered, \"sentiment_analysis_distilbert\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_distilbert\")\n",
    "\n",
    "result_score(Y_val, y_pred_roberta, \"Roberta\", binary=True)\n",
    "result_score(Y_val, y_pred_distilbert, \"DistilBERT\", binary=True)\n",
    "result_score(Y_val, y_pred_bert, \"BERT\", binary=False)\n",
    "result_score(Y_val, y_pred_bert_uncased, \"BERT Uncased\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_analysis_roberta</th>\n",
       "      <th>sentiment_analysis_bert</th>\n",
       "      <th>sentiment_analysis_distilbert</th>\n",
       "      <th>sentiment_analysis_bert_uncased</th>\n",
       "      <th>pred_sentiment_label_roberta</th>\n",
       "      <th>pred_sentiment_label_bert</th>\n",
       "      <th>pred_sentiment_label_bert_uncased</th>\n",
       "      <th>pred_sentiment_label_distilbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47252</td>\n",
       "      <td>Gabe</td>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.983961701393...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.7348992228507...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.768441677093...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9918521046638...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15710</td>\n",
       "      <td>Andy</td>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997746646404...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.4067431092262...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.993637084960...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9106979966163...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44150</td>\n",
       "      <td>Dwight</td>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998121678829...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.3897318840026...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.903421878814...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.5951375961303...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45628</td>\n",
       "      <td>Phyllis</td>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.993801534175...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.4273286163806...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998319804668...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9912397265434...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27785</td>\n",
       "      <td>Pam</td>\n",
       "      <td>Oh, damn. [Pam looks down at her salad] They'v...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.999482750892...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.6407026648521...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.994488358497...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.9335484504699...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  speaker                                          line_text  \\\n",
       "0  47252     Gabe  Ok, but once this starts, it's going to be mov...   \n",
       "1  15710     Andy                                              What?   \n",
       "2  44150   Dwight  Just a little announcement folks, remember, th...   \n",
       "3  45628  Phyllis  Is it true that you're making Dwight the manager?   \n",
       "4  27785      Pam  Oh, damn. [Pam looks down at her salad] They'v...   \n",
       "\n",
       "   Sentiment                         sentiment_analysis_roberta  \\\n",
       "0       -1.0  [{'label': 'POSITIVE', 'score': 0.983961701393...   \n",
       "1        0.0  [{'label': 'NEGATIVE', 'score': 0.997746646404...   \n",
       "2        1.0  [{'label': 'POSITIVE', 'score': 0.998121678829...   \n",
       "3       -1.0  [{'label': 'NEGATIVE', 'score': 0.993801534175...   \n",
       "4        0.0  [{'label': 'NEGATIVE', 'score': 0.999482750892...   \n",
       "\n",
       "                             sentiment_analysis_bert  \\\n",
       "0  [{'label': 'LABEL_0', 'score': 0.7348992228507...   \n",
       "1  [{'label': 'LABEL_0', 'score': 0.4067431092262...   \n",
       "2  [{'label': 'LABEL_0', 'score': 0.3897318840026...   \n",
       "3  [{'label': 'LABEL_1', 'score': 0.4273286163806...   \n",
       "4  [{'label': 'LABEL_0', 'score': 0.6407026648521...   \n",
       "\n",
       "                       sentiment_analysis_distilbert  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.768441677093...   \n",
       "1  [{'label': 'NEGATIVE', 'score': 0.993637084960...   \n",
       "2  [{'label': 'NEGATIVE', 'score': 0.903421878814...   \n",
       "3  [{'label': 'POSITIVE', 'score': 0.998319804668...   \n",
       "4  [{'label': 'NEGATIVE', 'score': 0.994488358497...   \n",
       "\n",
       "                     sentiment_analysis_bert_uncased  \\\n",
       "0  [{'label': 'LABEL_1', 'score': 0.9918521046638...   \n",
       "1  [{'label': 'LABEL_1', 'score': 0.9106979966163...   \n",
       "2  [{'label': 'LABEL_0', 'score': 0.5951375961303...   \n",
       "3  [{'label': 'LABEL_1', 'score': 0.9912397265434...   \n",
       "4  [{'label': 'LABEL_0', 'score': 0.9335484504699...   \n",
       "\n",
       "   pred_sentiment_label_roberta  pred_sentiment_label_bert  \\\n",
       "0                             1                         -1   \n",
       "1                            -1                         -1   \n",
       "2                             1                         -1   \n",
       "3                            -1                          0   \n",
       "4                            -1                         -1   \n",
       "\n",
       "   pred_sentiment_label_bert_uncased  pred_sentiment_label_distilbert  \n",
       "0                                  0                               -1  \n",
       "1                                  0                               -1  \n",
       "2                                 -1                               -1  \n",
       "3                                  0                                1  \n",
       "4                                 -1                               -1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a descent accuracy, but a major limitation is that the classifier predicts into two classes (either positive or negative), while we have 3 classes (positive, neutral, negative). I solved this by setting the neutral class labeled by us to positive, but this obviously reduces the accuracy of the model by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_filtered[[\"line_text\", \"Sentiment\", \"pred_sentiment_label_roberta\", \"pred_sentiment_label_bert\", \"pred_sentiment_label_bert_uncased\", \"pred_sentiment_label_distilbert\"]]\n",
    "\n",
    "# rename columns\n",
    "df_compare.columns = [\"line_text\", \"Annotated\", \"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to combine measures\n",
    "\n",
    "1. Voting: take the majority vote of the different sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis with Majority:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.4792626728110599          \n",
      "Precision: 0.5413793103448276          \n",
      "Recall: 0.6029208472686733          \n",
      "F1: 0.48141718106995884          \n",
      "MSE: 0.6728110599078341\n",
      "\n",
      "Analysis with Majority_minus_BERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.39631336405529954          \n",
      "Precision: 0.2768552311435523          \n",
      "Recall: 0.5612820512820513          \n",
      "F1: 0.3657848324514991          \n",
      "MSE: 0.8248847926267281\n",
      "\n",
      "MSE for Average:      \n",
      "- - - - - - - - - -      \n",
      "0.38623271889400923\n",
      "\n",
      "MSE for Average minus BERT:      \n",
      "- - - - - - - - - -      \n",
      "0.43317972350230416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# make a new column with the majority vote\n",
    "df_compare[\"Majority\"] = df_compare[[\"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]].mode(axis=1)[0]\n",
    "df_compare[\"Majority_minus_BERT\"] = df_compare[[\"Roberta\", \"Bert_uncased\", \"Distilbert\"]].mode(axis=1)[0]\n",
    "\n",
    "# make a new column with the average vote\n",
    "df_compare[\"Average\"] = df_compare[[\"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]].mean(axis=1)\n",
    "df_compare[\"Average_minus_BERT\"] = df_compare[[\"Roberta\", \"Bert_uncased\", \"Distilbert\"]].mean(axis=1)\n",
    "\n",
    "Y_pred_majority = df_compare[\"Majority\"].values\n",
    "Y_pred_majority_minus = df_compare[\"Majority_minus_BERT\"].values\n",
    "Y_pred_average = df_compare[\"Average\"].values\n",
    "Y_pred_average_minus = df_compare[\"Average_minus_BERT\"].values\n",
    "\n",
    "result_score(Y_val, Y_pred_majority, \"Majority\", binary=False)\n",
    "result_score(Y_val, Y_pred_majority_minus, \"Majority_minus_BERT\", binary=False)\n",
    "\n",
    "print(f\"MSE for Average:\\\n",
    "      \\n- - - - - - - - - -\\\n",
    "      \\n{mean_squared_error(Y_val, Y_pred_average)}\\n\")\n",
    "\n",
    "print(f\"MSE for Average minus BERT:\\\n",
    "      \\n- - - - - - - - - -\\\n",
    "      \\n{mean_squared_error(Y_val, Y_pred_average_minus)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_text</th>\n",
       "      <th>Annotated</th>\n",
       "      <th>Roberta</th>\n",
       "      <th>Bert</th>\n",
       "      <th>Bert_uncased</th>\n",
       "      <th>Distilbert</th>\n",
       "      <th>Majority</th>\n",
       "      <th>Majority_minus_BERT</th>\n",
       "      <th>Average</th>\n",
       "      <th>Average_minus_BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, damn. [Pam looks down at her salad] They'v...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           line_text  Annotated  Roberta  \\\n",
       "0  Ok, but once this starts, it's going to be mov...       -1.0        1   \n",
       "1                                              What?        0.0       -1   \n",
       "2  Just a little announcement folks, remember, th...        1.0        1   \n",
       "3  Is it true that you're making Dwight the manager?       -1.0       -1   \n",
       "4  Oh, damn. [Pam looks down at her salad] They'v...        0.0       -1   \n",
       "\n",
       "   Bert  Bert_uncased  Distilbert  Majority  Majority_minus_BERT  Average  \\\n",
       "0    -1             0          -1      -1.0                 -1.0    -0.25   \n",
       "1    -1             0          -1      -1.0                 -1.0    -0.75   \n",
       "2    -1            -1          -1      -1.0                 -1.0    -0.50   \n",
       "3     0             0           1       0.0                 -1.0     0.00   \n",
       "4    -1            -1          -1      -1.0                 -1.0    -1.00   \n",
       "\n",
       "   Average_minus_BERT  \n",
       "0            0.000000  \n",
       "1           -0.666667  \n",
       "2           -0.333333  \n",
       "3            0.000000  \n",
       "4           -1.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search to find best combination of pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to find the best combination of pipeline and model\n",
    "pipelines = [\"keep\", \"remove\", \"start\", \"end\"]\n",
    "models = {\n",
    "    \"Roberta\":sentiment_analysis_roberta,\n",
    "    \"Bert\":sentiment_analysis_bert,\n",
    "    \"Bert_uncased\":sentiment_analysis_bert_uncased,\n",
    "    \"Distilbert\":sentiment_analysis_distilbert\n",
    "    }\n",
    "# make dataframe to store results\n",
    "df_grid = pd.DataFrame(columns=[\"pipeline\", \"model\", \"Accuracy\", \"MSE\"])\n",
    "\n",
    "def grid_search(df, pipelines, models):\n",
    "    row = 0\n",
    "    for pipeline in pipelines:\n",
    "        df_piped = preprocess_sentiment(df, description_mode=pipeline)\n",
    "        for model in models:\n",
    "            fit_sentiment(df_piped, models[model], f\"{pipeline}, {model}\")\n",
    "            y_pred = \n",
    "            #acc = accuracy_score(Y_val, y_pred)\n",
    "            mse = mean_squared_error(Y_val, y_pred)\n",
    "            df_grid.loc[row] = [pipeline, model, 0, mse]\n",
    "            row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_annotated\n",
    "pipeline = pipelines[0]\n",
    "df_piped = preprocess_sentiment(df, description_mode=pipeline)\n",
    "model = \"Roberta\"\n",
    "fit_sentiment(df_piped, models[model], f\"{pipeline}, {model}\", ret=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>keep, Roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47252</td>\n",
       "      <td>Gabe</td>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9883946180343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15710</td>\n",
       "      <td>Andy</td>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.9947077035903...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44150</td>\n",
       "      <td>Dwight</td>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9913460612297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45628</td>\n",
       "      <td>Phyllis</td>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>{'label': 'POSITIVE', 'score': 0.993751585483551}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27785</td>\n",
       "      <td>Pam</td>\n",
       "      <td>Oh, damn. Pam looks down at her salad They've ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'label': 'NEGATIVE', 'score': 0.9994884729385...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  speaker                                          line_text  \\\n",
       "0  47252     Gabe  Ok, but once this starts, it's going to be mov...   \n",
       "1  15710     Andy                                              What?   \n",
       "2  44150   Dwight  Just a little announcement folks, remember, th...   \n",
       "3  45628  Phyllis  Is it true that you're making Dwight the manager?   \n",
       "4  27785      Pam  Oh, damn. Pam looks down at her salad They've ...   \n",
       "\n",
       "   Sentiment                                      keep, Roberta  \n",
       "0       -1.0  {'label': 'POSITIVE', 'score': 0.9883946180343...  \n",
       "1        0.0  {'label': 'POSITIVE', 'score': 0.9947077035903...  \n",
       "2        1.0  {'label': 'NEGATIVE', 'score': 0.9913460612297...  \n",
       "3       -1.0  {'label': 'POSITIVE', 'score': 0.993751585483551}  \n",
       "4        0.0  {'label': 'NEGATIVE', 'score': 0.9994884729385...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_piped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_piped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_345/4092450367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_piped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{pipeline}, {model}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrans_dict_roberta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_piped' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = df_piped[f\"{pipeline}, {model}\"].apply(lambda x: trans_dict_roberta[x[\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7f11af4fee10>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for model in models:\n",
    "    li.append(model)\n",
    "models[li[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis keep, Roberta\n",
      "sample 100 out of 217. 46.08%  \u001b[1K\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2742/798624172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_annotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2742/1768677379.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(df, pipelines, models)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_piped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{pipeline}, {model}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#acc = accuracy_score(Y_val, y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mdf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \"\"\"\n\u001b[1;32m    438\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "grid_search(df_annotated, pipelines, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
