{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "For the sentiment analysis, we tried out several different models and pre-processing pipelines. Especially for dealing with comments or descriptions in the lines, like [laughing] or [to camera], we tried out different methods to see which resulted in the best score for the sentiment analysis.\n",
    "\n",
    "We mainly used sentiment analysis based on pre-trained models, and then tested the accuracy by comparing the predicted sentiment with the sentiments given by us in the annotated sample (of 300 lines).\n",
    "\n",
    "## 1. Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'The_Office_lines.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_759/3266452271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The_Office_lines.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'The_Office_lines.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"The_Office_lines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\"id\",\"speaker\", \"line_text\"]\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# deals with descriptions in lines, e.g. [laughs] or [to camera]\n",
    "def deal_with_description(line, mode):\n",
    "    if mode==\"remove\":\n",
    "        # remove text that is between brackets\n",
    "        line = re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"end\":\n",
    "        # move all the text that is in the brackets to the end of the line\n",
    "        line = re.sub(r'\\[.*?\\]', '', line) + \" \" + \", \".join(re.findall(r\"\\[(.*?)\\]\", line))\n",
    "    elif mode==\"start\":\n",
    "        # move all the text that is in the brackets to the start of the line\n",
    "        line = \", \".join(re.findall(r\"\\[(.*?)\\]\", line)) + \" \" + re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"keep\":\n",
    "        # remove all brackets from the line but keep text in place\n",
    "        line = re.sub(r\"[\\([{})\\]]\", '', line)\n",
    "    return line\n",
    "\n",
    "def preprocess_sentiment(df, description_mode):\n",
    "    # deal with descriptions in lines\n",
    "    df_pre = df.copy()\n",
    "    df_pre[\"line_text\"] = df_pre[\"line_text\"].apply(lambda x: deal_with_description(x, mode=description_mode))\n",
    "    \n",
    "    return df_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the sentiment analysis first only the the sample labeled by us, and then applied the best performing combination of pipeline and model to the whole dataset.\n",
    "\n",
    "#### Function to extract ids that have been annotated by us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_ids():\n",
    "    df_luuk = pd.read_csv(\"annotated_data/sample_Luuk.csv\")\n",
    "    df_shan = pd.read_csv(\"annotated_data/sample_Shantanu.csv\")\n",
    "    df_elin = pd.read_csv(\"annotated_data/sample_Eline.csv\")\n",
    "\n",
    "    # combine annotations\n",
    "    df_combined = pd.concat([df_luuk, df_shan, df_elin], axis=0)\n",
    "\n",
    "    # filter out only columns that have something in \"Sentiment\" column\n",
    "    df_annotated = df_combined[df_combined[\"Sentiment\"].notna()]\n",
    "    df_annotated.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the accuracy of the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "# translating strings of sentiment to integers\n",
    "trans_dict_roberta = {\n",
    "    \"NEGATIVE\": -1,\n",
    "    \"POSITIVE\": 1\n",
    "}\n",
    "\n",
    "trans_dict_bert = {\n",
    "    \"LABEL_0\": -1,\n",
    "    \"LABEL_1\": 0,\n",
    "    \"LABEL_2\": 1\n",
    "}\n",
    "\n",
    "# extract predicted values from dataframe\n",
    "def extract_ypred(df, source_column, transdict, write=True, target_column=\"temp\"):\n",
    "    df[target_column] = df[source_column].apply(lambda x: transdict[x[0][\"label\"]])\n",
    "    Y_pred = df[target_column].values\n",
    "    if not write:\n",
    "        df = df.drop(columns=[target_column])\n",
    "    return Y_pred\n",
    "\n",
    "def result_score(Y_val, Y_pred, name, binary=False):\n",
    "    # make new list replacing 0 with 1 if binary\n",
    "    if binary:\n",
    "        Y_val_used = [1 if x==0 else x for x in Y_val]\n",
    "    else:\n",
    "        Y_val_used = Y_val\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(Y_val_used, Y_pred)\n",
    "    precision = precision_score(Y_val_used, Y_pred, average=\"macro\")\n",
    "    recall = recall_score(Y_val_used, Y_pred, average=\"macro\")\n",
    "    f1 = f1_score(Y_val_used, Y_pred, average=\"macro\")\n",
    "    MSE = mean_squared_error(Y_val_used, Y_pred)\n",
    "\n",
    "    # print results\n",
    "    print(f\"Analysis with {name}:\\\n",
    "          \\n- - - - - - - - - - \\\n",
    "          \\nAccuracy: {accuracy}\\\n",
    "          \\nPrecision: {precision}\\\n",
    "          \\nRecall: {recall}\\\n",
    "          \\nF1: {f1}\\\n",
    "          \\nMSE: {MSE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find current time\n",
    "import time\n",
    "#supress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fit_sentiment(df_filt, method, name, progress=True, ret=False):\n",
    "    # set start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # apply sentiment analysis to each line, track progress\n",
    "    df_filtered = df_filt.copy()\n",
    "    df_filtered[name] = \"\"\n",
    "\n",
    "    # apply sentiment analysis to each line and track progress\n",
    "    if progress:\n",
    "        print(f\"Fit sentiment analysis {name}\")\n",
    "    k = len(df_filtered)\n",
    "    i = 0\n",
    "    for iter, row in df_filtered.iterrows():\n",
    "        df_filtered[name][iter] = method(row[\"line_text\"])\n",
    "        if progress:\n",
    "            print(f\"sample {iter+1} out of {k}. {round((iter+1)/k*100, 2)}%  \", end='\\x1b[1K\\r')\n",
    "        i += 1\n",
    "    if ret:\n",
    "        #drop name column\n",
    "        y_pred = df_filtered[name].values\n",
    "        df_filtered = df_filtered.drop(columns=[name])\n",
    "        return y_pred\n",
    "    else:\n",
    "        df_filt[name] = df_filtered[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import first pre-trained sentiment analysis pipeline\n",
    "from transformers import pipeline\n",
    "sentiment_analysis_roberta = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "sentiment_analysis_bert  = pipeline(\"sentiment-analysis\",model=\"sbcBI/sentiment_analysis_model\")\n",
    "sentiment_analysis_distilbert = pipeline(\"sentiment-analysis\",model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_analysis_bert_uncased = pipeline(\"sentiment-analysis\",model=\"Seethal/sentiment_analysis_generic_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_distilbert(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9952951073646545}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_bert_uncased(\"I hate you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, for now, take only the lines that have been annotated by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only annotated lines\n",
    "df_annotated = annotated_ids()\n",
    "\n",
    "# preprocess the lines\n",
    "df_filtered = preprocess_sentiment(df_annotated, description_mode=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis sentiment_analysis_roberta\n",
      "Fit sentiment analysis sentiment_analysis_bert\n",
      "Fit sentiment analysis sentiment_analysis_distilbert\n",
      "Fit sentiment analysis sentiment_analysis_bert_uncased\n",
      "sample 300 out of 300. 100.0%  \u001b[1K\r"
     ]
    }
   ],
   "source": [
    "fit_sentiment(df_filtered, sentiment_analysis_roberta, \"sentiment_analysis_roberta\", )\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert, \"sentiment_analysis_bert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_distilbert, \"sentiment_analysis_distilbert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert_uncased, \"sentiment_analysis_bert_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis with Roberta:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6533333333333333          \n",
      "Precision: 0.6769065424745868          \n",
      "Recall: 0.7383984068553497          \n",
      "F1: 0.6368038740920097          \n",
      "MSE: 1.3866666666666667\n",
      "\n",
      "Analysis with DistilBERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.64          \n",
      "Precision: 0.655961461260538          \n",
      "Recall: 0.711001146581377          \n",
      "F1: 0.6205022488755623          \n",
      "MSE: 1.44\n",
      "\n",
      "Analysis with BERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.55          \n",
      "Precision: 0.5586803842381936          \n",
      "Recall: 0.5925873014044264          \n",
      "F1: 0.5535982008995503          \n",
      "MSE: 0.66\n",
      "\n",
      "Analysis with BERT Uncased:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6266666666666667          \n",
      "Precision: 0.6295574458267306          \n",
      "Recall: 0.605125369156384          \n",
      "F1: 0.6125925925925926          \n",
      "MSE: 0.47333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get values of Y_val\n",
    "Y_val = df_filtered[\"Sentiment\"].values\n",
    "\n",
    "y_pred_roberta = extract_ypred(df_filtered, \"sentiment_analysis_roberta\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_roberta\")\n",
    "y_pred_bert = extract_ypred(df_filtered, \"sentiment_analysis_bert\", trans_dict_bert,  write=True, target_column=\"pred_sentiment_label_bert\")\n",
    "y_pred_bert_uncased = extract_ypred(df_filtered, \"sentiment_analysis_bert_uncased\", trans_dict_bert, write=True, target_column=\"pred_sentiment_label_bert_uncased\")\n",
    "y_pred_distilbert = extract_ypred(df_filtered, \"sentiment_analysis_distilbert\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_distilbert\")\n",
    "\n",
    "result_score(Y_val, y_pred_roberta, \"Roberta\", binary=True)\n",
    "result_score(Y_val, y_pred_distilbert, \"DistilBERT\", binary=True)\n",
    "result_score(Y_val, y_pred_bert, \"BERT\", binary=False)\n",
    "result_score(Y_val, y_pred_bert_uncased, \"BERT Uncased\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_analysis_roberta</th>\n",
       "      <th>sentiment_analysis_bert</th>\n",
       "      <th>sentiment_analysis_distilbert</th>\n",
       "      <th>sentiment_analysis_bert_uncased</th>\n",
       "      <th>pred_sentiment_label_roberta</th>\n",
       "      <th>pred_sentiment_label_bert</th>\n",
       "      <th>pred_sentiment_label_bert_uncased</th>\n",
       "      <th>pred_sentiment_label_distilbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47252</td>\n",
       "      <td>Gabe</td>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.983961701393...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.7348992228507...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.768441677093...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9918521046638...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15710</td>\n",
       "      <td>Andy</td>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997746646404...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.4067431092262...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.993637084960...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9106979966163...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44150</td>\n",
       "      <td>Dwight</td>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998121678829...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.3897318840026...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.903421878814...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.5951375961303...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45628</td>\n",
       "      <td>Phyllis</td>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.993801534175...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.4273286163806...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998319804668...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9912397265434...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27785</td>\n",
       "      <td>Pam</td>\n",
       "      <td>Oh, damn. Pam looks down at her salad They've ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.999487280845...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.6776348948478...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.991238892078...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.9609617590904...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  speaker                                          line_text  \\\n",
       "0  47252     Gabe  Ok, but once this starts, it's going to be mov...   \n",
       "1  15710     Andy                                              What?   \n",
       "2  44150   Dwight  Just a little announcement folks, remember, th...   \n",
       "3  45628  Phyllis  Is it true that you're making Dwight the manager?   \n",
       "4  27785      Pam  Oh, damn. Pam looks down at her salad They've ...   \n",
       "\n",
       "   Sentiment                         sentiment_analysis_roberta  \\\n",
       "0       -1.0  [{'label': 'POSITIVE', 'score': 0.983961701393...   \n",
       "1        0.0  [{'label': 'NEGATIVE', 'score': 0.997746646404...   \n",
       "2        1.0  [{'label': 'POSITIVE', 'score': 0.998121678829...   \n",
       "3       -1.0  [{'label': 'NEGATIVE', 'score': 0.993801534175...   \n",
       "4        0.0  [{'label': 'NEGATIVE', 'score': 0.999487280845...   \n",
       "\n",
       "                             sentiment_analysis_bert  \\\n",
       "0  [{'label': 'LABEL_0', 'score': 0.7348992228507...   \n",
       "1  [{'label': 'LABEL_0', 'score': 0.4067431092262...   \n",
       "2  [{'label': 'LABEL_0', 'score': 0.3897318840026...   \n",
       "3  [{'label': 'LABEL_1', 'score': 0.4273286163806...   \n",
       "4  [{'label': 'LABEL_0', 'score': 0.6776348948478...   \n",
       "\n",
       "                       sentiment_analysis_distilbert  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.768441677093...   \n",
       "1  [{'label': 'NEGATIVE', 'score': 0.993637084960...   \n",
       "2  [{'label': 'NEGATIVE', 'score': 0.903421878814...   \n",
       "3  [{'label': 'POSITIVE', 'score': 0.998319804668...   \n",
       "4  [{'label': 'NEGATIVE', 'score': 0.991238892078...   \n",
       "\n",
       "                     sentiment_analysis_bert_uncased  \\\n",
       "0  [{'label': 'LABEL_1', 'score': 0.9918521046638...   \n",
       "1  [{'label': 'LABEL_1', 'score': 0.9106979966163...   \n",
       "2  [{'label': 'LABEL_0', 'score': 0.5951375961303...   \n",
       "3  [{'label': 'LABEL_1', 'score': 0.9912397265434...   \n",
       "4  [{'label': 'LABEL_0', 'score': 0.9609617590904...   \n",
       "\n",
       "   pred_sentiment_label_roberta  pred_sentiment_label_bert  \\\n",
       "0                             1                         -1   \n",
       "1                            -1                         -1   \n",
       "2                             1                         -1   \n",
       "3                            -1                          0   \n",
       "4                            -1                         -1   \n",
       "\n",
       "   pred_sentiment_label_bert_uncased  pred_sentiment_label_distilbert  \n",
       "0                                  0                               -1  \n",
       "1                                  0                               -1  \n",
       "2                                 -1                               -1  \n",
       "3                                  0                                1  \n",
       "4                                 -1                               -1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a descent accuracy, but a major limitation is that the classifier predicts into two classes (either positive or negative), while we have 3 classes (positive, neutral, negative). I solved this by setting the neutral class labeled by us to positive, but this obviously reduces the accuracy of the model by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_filtered[[\"line_text\", \"Sentiment\", \"pred_sentiment_label_roberta\", \"pred_sentiment_label_bert\", \"pred_sentiment_label_bert_uncased\", \"pred_sentiment_label_distilbert\"]]\n",
    "\n",
    "# rename columns\n",
    "df_compare.columns = [\"line_text\", \"Annotated\", \"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to combine measures\n",
    "\n",
    "1. Voting: take the majority vote of the different sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis with Majority:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.5066666666666667          \n",
      "Precision: 0.5630272051840679          \n",
      "Recall: 0.6010088674339743          \n",
      "F1: 0.500207232201319          \n",
      "MSE: 0.6933333333333334\n",
      "\n",
      "Analysis with Majority_minus_BERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.43          \n",
      "Precision: 0.3011531241619737          \n",
      "Recall: 0.5584697627798195          \n",
      "F1: 0.3829059829059829          \n",
      "MSE: 0.83\n",
      "\n",
      "MSE for Average:      \n",
      "- - - - - - - - - -      \n",
      "0.39875\n",
      "\n",
      "MSE for Average minus BERT:      \n",
      "- - - - - - - - - -      \n",
      "0.4288888888888889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# make a new column with the majority vote\n",
    "df_compare[\"Majority\"] = df_compare[[\"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]].mode(axis=1)[0]\n",
    "df_compare[\"Majority_minus_BERT\"] = df_compare[[\"Roberta\", \"Bert_uncased\", \"Distilbert\"]].mode(axis=1)[0]\n",
    "\n",
    "# make a new column with the average vote\n",
    "df_compare[\"Average\"] = df_compare[[\"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]].mean(axis=1)\n",
    "df_compare[\"Average_minus_BERT\"] = df_compare[[\"Roberta\", \"Bert_uncased\", \"Distilbert\"]].mean(axis=1)\n",
    "\n",
    "Y_pred_majority = df_compare[\"Majority\"].values\n",
    "Y_pred_majority_minus = df_compare[\"Majority_minus_BERT\"].values\n",
    "Y_pred_average = df_compare[\"Average\"].values\n",
    "Y_pred_average_minus = df_compare[\"Average_minus_BERT\"].values\n",
    "\n",
    "result_score(Y_val, Y_pred_majority, \"Majority\", binary=False)\n",
    "result_score(Y_val, Y_pred_majority_minus, \"Majority_minus_BERT\", binary=False)\n",
    "\n",
    "print(f\"MSE for Average:\\\n",
    "      \\n- - - - - - - - - -\\\n",
    "      \\n{mean_squared_error(Y_val, Y_pred_average)}\\n\")\n",
    "\n",
    "print(f\"MSE for Average minus BERT:\\\n",
    "      \\n- - - - - - - - - -\\\n",
    "      \\n{mean_squared_error(Y_val, Y_pred_average_minus)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_text</th>\n",
       "      <th>Annotated</th>\n",
       "      <th>Roberta</th>\n",
       "      <th>Bert</th>\n",
       "      <th>Bert_uncased</th>\n",
       "      <th>Distilbert</th>\n",
       "      <th>Majority</th>\n",
       "      <th>Majority_minus_BERT</th>\n",
       "      <th>Average</th>\n",
       "      <th>Average_minus_BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, damn. Pam looks down at her salad They've ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           line_text  Annotated  Roberta  \\\n",
       "0  Ok, but once this starts, it's going to be mov...       -1.0        1   \n",
       "1                                              What?        0.0       -1   \n",
       "2  Just a little announcement folks, remember, th...        1.0        1   \n",
       "3  Is it true that you're making Dwight the manager?       -1.0       -1   \n",
       "4  Oh, damn. Pam looks down at her salad They've ...        0.0       -1   \n",
       "\n",
       "   Bert  Bert_uncased  Distilbert  Majority  Majority_minus_BERT  Average  \\\n",
       "0    -1             0          -1      -1.0                 -1.0    -0.25   \n",
       "1    -1             0          -1      -1.0                 -1.0    -0.75   \n",
       "2    -1            -1          -1      -1.0                 -1.0    -0.50   \n",
       "3     0             0           1       0.0                 -1.0     0.00   \n",
       "4    -1            -1          -1      -1.0                 -1.0    -1.00   \n",
       "\n",
       "   Average_minus_BERT  \n",
       "0            0.000000  \n",
       "1           -0.666667  \n",
       "2           -0.333333  \n",
       "3            0.000000  \n",
       "4           -1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search to find best combination of pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to find the best combination of pipeline and model\n",
    "pipelines = [\"keep\", \"remove\", \"start\", \"end\"]\n",
    "models = {\n",
    "    \"Roberta\":sentiment_analysis_roberta,\n",
    "    \"Bert\":sentiment_analysis_bert,\n",
    "    \"Bert_uncased\":sentiment_analysis_bert_uncased,\n",
    "    \"Distilbert\":sentiment_analysis_distilbert\n",
    "    }\n",
    "trans_dict_comb = {\n",
    "    \"NEGATIVE\": -1,\n",
    "    \"POSITIVE\": 1,\n",
    "    \"LABEL_0\": -1,\n",
    "    \"LABEL_1\": 0,\n",
    "    \"LABEL_2\": 1\n",
    "}\n",
    "# make dataframe to store results\n",
    "df_grid = pd.DataFrame(columns=[\"pipeline\", \"model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"MSE\"])\n",
    "\n",
    "def grid_search(df, pipelines, models):\n",
    "    row = 0\n",
    "    for pipeline in pipelines:\n",
    "        df_piped = preprocess_sentiment(df, description_mode=pipeline)\n",
    "        for model in models:\n",
    "            fit_sentiment(df_piped, models[model], f\"{pipeline}, {model}\")\n",
    "            y_pred = extract_ypred(df_piped, f\"{pipeline}, {model}\", trans_dict_comb, write=False)\n",
    "            if model in [\"Roberta\", \"Disilbert\"]:\n",
    "                Y_val_used = [1 if x==0 else x for x in Y_val]\n",
    "            else:\n",
    "                Y_val_used = Y_val\n",
    "            acc = accuracy_score(Y_val_used, y_pred)\n",
    "            precision = precision_score(Y_val_used, y_pred, average=\"macro\")\n",
    "            recall = recall_score(Y_val_used, y_pred, average=\"macro\")\n",
    "            f1 = f1_score(Y_val_used, y_pred, average=\"macro\")\n",
    "            mse = mean_squared_error(Y_val_used, y_pred)\n",
    "            df_grid.loc[row] = [pipeline, model, acc, precision, recall, f1, mse]\n",
    "            row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis keep, Roberta\n",
      "Fit sentiment analysis keep, Bert1K\n",
      "Fit sentiment analysis keep, Bert_uncased\n",
      "Fit sentiment analysis keep, Distilbert\n",
      "Fit sentiment analysis remove, Roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis remove, Bert\n",
      "Fit sentiment analysis remove, Bert_uncased\n",
      "Fit sentiment analysis remove, Distilbert\n",
      "sample 300 out of 300. 100.0%  \u001b[1K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis start, Roberta\n",
      "Fit sentiment analysis start, BertK\n",
      "Fit sentiment analysis start, Bert_uncased\n",
      "Fit sentiment analysis start, Distilbert\n",
      "Fit sentiment analysis end, Roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis end, Bert[1K\n",
      "Fit sentiment analysis end, Bert_uncased\n",
      "Fit sentiment analysis end, Distilbert\n",
      "sample 300 out of 300. 100.0%  \u001b[1K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "grid_search(df_annotated, pipelines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>remove</td>\n",
       "      <td>Bert_uncased</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.634672</td>\n",
       "      <td>0.609723</td>\n",
       "      <td>0.617883</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keep</td>\n",
       "      <td>Bert_uncased</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.629557</td>\n",
       "      <td>0.605125</td>\n",
       "      <td>0.612593</td>\n",
       "      <td>0.473333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>end</td>\n",
       "      <td>Bert_uncased</td>\n",
       "      <td>0.623333</td>\n",
       "      <td>0.626544</td>\n",
       "      <td>0.598793</td>\n",
       "      <td>0.607373</td>\n",
       "      <td>0.476667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>start</td>\n",
       "      <td>Bert_uncased</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.619408</td>\n",
       "      <td>0.598260</td>\n",
       "      <td>0.605123</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>remove</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.543333</td>\n",
       "      <td>0.546398</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>0.546870</td>\n",
       "      <td>0.656667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>end</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.561230</td>\n",
       "      <td>0.594385</td>\n",
       "      <td>0.556685</td>\n",
       "      <td>0.656667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keep</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.558680</td>\n",
       "      <td>0.592587</td>\n",
       "      <td>0.553598</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>start</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.543333</td>\n",
       "      <td>0.551262</td>\n",
       "      <td>0.585722</td>\n",
       "      <td>0.547028</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>start</td>\n",
       "      <td>Distilbert</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.285481</td>\n",
       "      <td>0.551398</td>\n",
       "      <td>0.375266</td>\n",
       "      <td>0.843333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>keep</td>\n",
       "      <td>Distilbert</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.283643</td>\n",
       "      <td>0.547333</td>\n",
       "      <td>0.372481</td>\n",
       "      <td>0.856667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>end</td>\n",
       "      <td>Distilbert</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.281027</td>\n",
       "      <td>0.542766</td>\n",
       "      <td>0.369404</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>remove</td>\n",
       "      <td>Distilbert</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.275989</td>\n",
       "      <td>0.534135</td>\n",
       "      <td>0.363346</td>\n",
       "      <td>0.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remove</td>\n",
       "      <td>Roberta</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.690899</td>\n",
       "      <td>0.758464</td>\n",
       "      <td>0.658607</td>\n",
       "      <td>1.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keep</td>\n",
       "      <td>Roberta</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.676907</td>\n",
       "      <td>0.738398</td>\n",
       "      <td>0.636804</td>\n",
       "      <td>1.386667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>start</td>\n",
       "      <td>Roberta</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.671632</td>\n",
       "      <td>0.731549</td>\n",
       "      <td>0.632760</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>end</td>\n",
       "      <td>Roberta</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.673982</td>\n",
       "      <td>0.732032</td>\n",
       "      <td>0.623059</td>\n",
       "      <td>1.453333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pipeline         model  Accuracy  Precision    Recall        F1       MSE\n",
       "6    remove  Bert_uncased  0.633333   0.634672  0.609723  0.617883  0.466667\n",
       "2      keep  Bert_uncased  0.626667   0.629557  0.605125  0.612593  0.473333\n",
       "14      end  Bert_uncased  0.623333   0.626544  0.598793  0.607373  0.476667\n",
       "10    start  Bert_uncased  0.620000   0.619408  0.598260  0.605123  0.490000\n",
       "5    remove          Bert  0.543333   0.546398  0.578920  0.546870  0.656667\n",
       "13      end          Bert  0.553333   0.561230  0.594385  0.556685  0.656667\n",
       "1      keep          Bert  0.550000   0.558680  0.592587  0.553598  0.660000\n",
       "9     start          Bert  0.543333   0.551262  0.585722  0.547028  0.666667\n",
       "11    start    Distilbert  0.426667   0.285481  0.551398  0.375266  0.843333\n",
       "3      keep    Distilbert  0.423333   0.283643  0.547333  0.372481  0.856667\n",
       "15      end    Distilbert  0.420000   0.281027  0.542766  0.369404  0.870000\n",
       "7    remove    Distilbert  0.413333   0.275989  0.534135  0.363346  0.896667\n",
       "4    remove       Roberta  0.676667   0.690899  0.758464  0.658607  1.293333\n",
       "0      keep       Roberta  0.653333   0.676907  0.738398  0.636804  1.386667\n",
       "8     start       Roberta  0.650000   0.671632  0.731549  0.632760  1.400000\n",
       "12      end       Roberta  0.636667   0.673982  0.732032  0.623059  1.453333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid.sort_values(by=['MSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we find that Bert_uncased is in fact the best model for this task. Even though the accuracy is slightly lower than the Roberta model, the MSE is way lower. Since we are trying to model sentiment analysis over a longer period of time, and take the average, A lower MSE is more important than a slightly higher accuracy.\n",
    "\n",
    "### Applying best model to entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in full dataset\n",
    "df_final = pd.read_csv(\"The_Office_lines.csv\")\n",
    "# preprocess line_text\n",
    "df_final[\"preprocessed\"] = df_final[\"line_text\"].apply(lambda x: deal_with_description(x, mode=\"remove\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis BERT_uncased_raw\n",
      "sample 59909 out of 59909. 100.0%  \u001b[1K\r"
     ]
    }
   ],
   "source": [
    "# fit sentiment\n",
    "fit_sentiment(df_final, sentiment_analysis_bert_uncased, \"BERT_uncased_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  0, ...,  1, -1,  1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to -1, 0, 1\n",
    "extract_ypred(df_final, \"BERT_uncased_raw\", trans_dict_comb, write=True, target_column=\"BERT_uncased_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>line_text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>deleted</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>BERT_uncased_raw</th>\n",
       "      <th>BERT_uncased_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "      <td>Michael</td>\n",
       "      <td>False</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "      <td>[{'label': 'LABEL_2', 'score': 0.9752480983734...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>Jim</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.6630714535713...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So you've come to the master for guidance? Is ...</td>\n",
       "      <td>Michael</td>\n",
       "      <td>False</td>\n",
       "      <td>So you've come to the master for guidance? Is ...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9957032799720...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "      <td>Jim</td>\n",
       "      <td>False</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9955984354019...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "      <td>Michael</td>\n",
       "      <td>False</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9973990917205...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  season  episode  scene  \\\n",
       "0   1       1        1      1   \n",
       "1   2       1        1      1   \n",
       "2   3       1        1      1   \n",
       "3   4       1        1      1   \n",
       "4   5       1        1      1   \n",
       "\n",
       "                                           line_text  speaker  deleted  \\\n",
       "0  All right Jim. Your quarterlies look very good...  Michael    False   \n",
       "1         Oh, I told you. I couldn't close it. So...      Jim    False   \n",
       "2  So you've come to the master for guidance? Is ...  Michael    False   \n",
       "3         Actually, you called me in here, but yeah.      Jim    False   \n",
       "4    All right. Well, let me show you how it's done.  Michael    False   \n",
       "\n",
       "                                        preprocessed  \\\n",
       "0  All right Jim. Your quarterlies look very good...   \n",
       "1         Oh, I told you. I couldn't close it. So...   \n",
       "2  So you've come to the master for guidance? Is ...   \n",
       "3         Actually, you called me in here, but yeah.   \n",
       "4    All right. Well, let me show you how it's done.   \n",
       "\n",
       "                                    BERT_uncased_raw  BERT_uncased_sentiment  \n",
       "0  [{'label': 'LABEL_2', 'score': 0.9752480983734...                       1  \n",
       "1  [{'label': 'LABEL_1', 'score': 0.6630714535713...                       0  \n",
       "2  [{'label': 'LABEL_1', 'score': 0.9957032799720...                       0  \n",
       "3  [{'label': 'LABEL_1', 'score': 0.9955984354019...                       0  \n",
       "4  [{'label': 'LABEL_1', 'score': 0.9973990917205...                       0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59909"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save annotated file\n",
    "df_final.to_csv(f\"sample_{annotators[i]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sentiment_labeled data\n",
    "df_sentiment = pd.read_csv(\"Sentiment_labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERT_uncased_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.216061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BERT_uncased_sentiment\n",
       " 0                0.583986\n",
       "-1                0.216061\n",
       " 1                0.199953"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list value counts of BERT_uncased_sentiment column, make into table with percentages\n",
    "df_sentiment[\"BERT_uncased_sentiment\"].value_counts(normalize=True).to_frame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
