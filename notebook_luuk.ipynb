{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "For the sentiment analysis, we tried out several different models and pre-processing pipelines. Especially for dealing with comments or descriptions in the lines, like [laughing] or [to camera], we tried out different methods to see which resulted in the best score for the sentiment analysis.\n",
    "\n",
    "We mainly used sentiment analysis based on pre-trained models, and then tested the accuracy by comparing the predicted sentiment with the sentiments given by us in the annotated sample (of 300 lines).\n",
    "\n",
    "## 1. Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"The_Office_lines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\"id\",\"speaker\", \"line_text\"]\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# deals with descriptions in lines, e.g. [laughs] or [to camera]\n",
    "def deal_with_description(line, mode):\n",
    "    if mode==\"remove\":\n",
    "        # remove text that is between brackets\n",
    "        line = re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"end\":\n",
    "        # move all the text that is in the brackets to the end of the line\n",
    "        line = re.sub(r'\\[.*?\\]', '', line) + \" \" + \", \".join(re.findall(r\"\\[(.*?)\\]\", line))\n",
    "    elif mode==\"start\":\n",
    "        # move all the text that is in the brackets to the start of the line\n",
    "        line = \", \".join(re.findall(r\"\\[(.*?)\\]\", line)) + \" \" + re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"keep\":\n",
    "        # remove all brackets from the line but keep text in place\n",
    "        line = re.sub(r\"[\\([{})\\]]\", '', line)\n",
    "    return line\n",
    "\n",
    "def preprocess_sentiment(df, relevant_columns, description_mode):\n",
    "    # filter out relevant columns\n",
    "    df = df[relevant_columns]\n",
    "    # deal with descriptions in lines\n",
    "    df[\"line_text\"] = df[\"line_text\"].apply(lambda x: deal_with_description(x, mode=description_mode))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = preprocess_sentiment(df, relevant_columns, description_mode=\"keep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the sentiment analysis first only the the sample labeled by us, and then applied the best performing combination of pipeline and model to the whole dataset.\n",
    "\n",
    "#### Function to extract ids that have been annotated by us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_ids():\n",
    "    df_luuk = pd.read_csv(\"annotated_data/sample_Luuk.csv\")\n",
    "    df_shan = pd.read_csv(\"annotated_data/sample_Shantanu.csv\")\n",
    "    df_elin = pd.read_csv(\"annotated_data/sample_Eline.csv\")\n",
    "\n",
    "    # combine annotations\n",
    "    df_combined = pd.concat([df_luuk, df_shan, df_elin], axis=0)\n",
    "\n",
    "    # filter out only columns that have something in \"Sentiment\" column\n",
    "    df_annotated = df_combined[df_combined[\"Sentiment\"].notna()]\n",
    "\n",
    "    return df_annotated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the accuracy of the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# translating strings of sentiment to integers\n",
    "trans_dict = {\n",
    "    \"NEGATIVE\": -1,\n",
    "    \"POSITIVE\": 1\n",
    "}\n",
    "\n",
    "# extract predicted values from dataframe\n",
    "def extract_ypred(df, source_column,  write=True, target_column=\"temp\"):\n",
    "    df[target_column] = df[source_column].apply(lambda x: trans_dict[x[0][\"label\"]])\n",
    "    Y_pred = df[target_column].values\n",
    "    if not write:\n",
    "        df = df.drop(columns=[target_column])\n",
    "    return Y_pred\n",
    "\n",
    "def result_score(Y_val, Y_pred, binary=False):\n",
    "    # replace neutral values with positive\n",
    "    if binary:\n",
    "        Y_val[Y_val==0] = 1\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(Y_val, Y_pred)\n",
    "    precision = precision_score(Y_val, Y_pred, average=\"macro\")\n",
    "    recall = recall_score(Y_val, Y_pred, average=\"macro\")\n",
    "    f1 = f1_score(Y_val, Y_pred, average=\"macro\")\n",
    "\n",
    "    # print results\n",
    "    print(f\"Accuracy: {accuracy}\\\n",
    "          \\nPrecision: {precision}\\\n",
    "          \\nRecall: {recall}\\\n",
    "          \\nF1: {f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find current time\n",
    "import time\n",
    "#supress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fit_sentiment(df_filtered, method, name, progress=True):\n",
    "    # set start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # apply sentiment analysis to each line, track progress\n",
    "    df_filtered[name] = \"\"\n",
    "\n",
    "    # apply sentiment analysis to each line and track progress\n",
    "    if progress:\n",
    "        print(f\"Fit sentiment analysis {name}\")\n",
    "    k = len(df_filtered)\n",
    "    for iter, row in df_filtered.iterrows():\n",
    "        df_filtered[name][iter] = method(row[\"line_text\"])\n",
    "        if progress:\n",
    "            print(f\"sample {iter} out of {k}. {round(iter/k*100, 2)}%\", end='\\x1b[1K\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import first pre-trained sentiment analysis pipeline\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, for now, take only the lines that have been annotated by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only annotated lines\n",
    "df_filtered = annotated_ids()\n",
    "# reset index\n",
    "df_filtered = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis sentiment_analysis\n",
      "sample 216 out of 217. 99.54%\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "fit_sentiment(df_filtered, sentiment_analysis, \"sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6359447004608295          \n",
      "Precision: 0.6626650660264106          \n",
      "Recall: 0.7210955710955711          \n",
      "F1: 0.6188157338847753\n"
     ]
    }
   ],
   "source": [
    "# get values of Y_val\n",
    "Y_val = df_filtered[\"Sentiment\"].values\n",
    "\n",
    "y_pred = extract_ypred(df_filtered, \"sentiment_analysis\", write=True, target_column=\"pred_sentiment_label\")\n",
    "result_score(Y_val, y_pred, binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
