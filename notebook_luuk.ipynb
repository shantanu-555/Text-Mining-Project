{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "For the sentiment analysis, we tried out several different models and pre-processing pipelines. Especially for dealing with comments or descriptions in the lines, like [laughing] or [to camera], we tried out different methods to see which resulted in the best score for the sentiment analysis.\n",
    "\n",
    "We mainly used sentiment analysis based on pre-trained models, and then tested the accuracy by comparing the predicted sentiment with the sentiments given by us in the annotated sample (of 300 lines).\n",
    "\n",
    "## 1. Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"The_Office_lines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\"id\",\"speaker\", \"line_text\"]\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# deals with descriptions in lines, e.g. [laughs] or [to camera]\n",
    "def deal_with_description(line, mode):\n",
    "    if mode==\"remove\":\n",
    "        # remove text that is between brackets\n",
    "        line = re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"end\":\n",
    "        # move all the text that is in the brackets to the end of the line\n",
    "        line = re.sub(r'\\[.*?\\]', '', line) + \" \" + \", \".join(re.findall(r\"\\[(.*?)\\]\", line))\n",
    "    elif mode==\"start\":\n",
    "        # move all the text that is in the brackets to the start of the line\n",
    "        line = \", \".join(re.findall(r\"\\[(.*?)\\]\", line)) + \" \" + re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"keep\":\n",
    "        # remove all brackets from the line but keep text in place\n",
    "        line = re.sub(r\"[\\([{})\\]]\", '', line)\n",
    "    return line\n",
    "\n",
    "def preprocess_sentiment(df, relevant_columns, description_mode):\n",
    "    # filter out relevant columns\n",
    "    df = df[relevant_columns]\n",
    "    # deal with descriptions in lines\n",
    "    df[\"line_text\"] = df[\"line_text\"].apply(lambda x: deal_with_description(x, mode=description_mode))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = preprocess_sentiment(df, relevant_columns, description_mode=\"keep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the sentiment analysis first only the the sample labeled by us, and then applied the best performing combination of pipeline and model to the whole dataset.\n",
    "\n",
    "#### Function to extract ids that have been annotated by us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_ids():\n",
    "    df_luuk = pd.read_csv(\"annotated_data/sample_Luuk.csv\")\n",
    "    df_shan = pd.read_csv(\"annotated_data/sample_Shantanu.csv\")\n",
    "    df_elin = pd.read_csv(\"annotated_data/sample_Eline.csv\")\n",
    "\n",
    "    # combine annotations\n",
    "    df_combined = pd.concat([df_luuk, df_shan, df_elin], axis=0)\n",
    "\n",
    "    # filter out only columns that have something in \"Sentiment\" column\n",
    "    df_annotated = df_combined[df_combined[\"Sentiment\"].notna()]\n",
    "\n",
    "    return df_annotated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the accuracy of the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# translating strings of sentiment to integers\n",
    "trans_dict_roberta = {\n",
    "    \"NEGATIVE\": -1,\n",
    "    \"POSITIVE\": 1\n",
    "}\n",
    "\n",
    "trans_dict_bert = {\n",
    "    \"LABEL_0\": -1,\n",
    "    \"LABEL_1\": 0,\n",
    "    \"LABEL_2\": 1\n",
    "}\n",
    "\n",
    "# extract predicted values from dataframe\n",
    "def extract_ypred(df, source_column, transdict, write=True, target_column=\"temp\"):\n",
    "    df[target_column] = df[source_column].apply(lambda x: transdict[x[0][\"label\"]])\n",
    "    Y_pred = df[target_column].values\n",
    "    if not write:\n",
    "        df = df.drop(columns=[target_column])\n",
    "    return Y_pred\n",
    "\n",
    "def result_score(Y_val, Y_pred, name, binary=False):\n",
    "    # make new list replacing 0 with 1 if binary\n",
    "    if binary:\n",
    "        Y_val = [1 if x==0 else x for x in Y_val]\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(Y_val, Y_pred)\n",
    "    precision = precision_score(Y_val, Y_pred, average=\"macro\")\n",
    "    recall = recall_score(Y_val, Y_pred, average=\"macro\")\n",
    "    f1 = f1_score(Y_val, Y_pred, average=\"macro\")\n",
    "\n",
    "    # print results\n",
    "    print(f\"Analysis with {name}:\\\n",
    "          \\n- - - - - - - - - - \\\n",
    "          \\nAccuracy: {accuracy}\\\n",
    "          \\nPrecision: {precision}\\\n",
    "          \\nRecall: {recall}\\\n",
    "          \\nF1: {f1}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find current time\n",
    "import time\n",
    "#supress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fit_sentiment(df_filtered, method, name, progress=True):\n",
    "    # set start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # apply sentiment analysis to each line, track progress\n",
    "    df_filtered[name] = \"\"\n",
    "\n",
    "    # apply sentiment analysis to each line and track progress\n",
    "    if progress:\n",
    "        print(f\"Fit sentiment analysis {name}\")\n",
    "    k = len(df_filtered)\n",
    "    for iter, row in df_filtered.iterrows():\n",
    "        df_filtered[name][iter] = method(row[\"line_text\"])\n",
    "        if progress:\n",
    "            print(f\"sample {iter+1} out of {k}. {round(iter+1/k*100, 2)}%\", end='\\x1b[1K\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import first pre-trained sentiment analysis pipeline\n",
    "from transformers import pipeline\n",
    "sentiment_analysis_roberta = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "sentiment_analysis_bert  = pipeline(\"sentiment-analysis\",model=\"sbcBI/sentiment_analysis_model\")\n",
    "sentiment_analysis_distilbert = pipeline(\"sentiment-analysis\",model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_analysis_bert_uncased = pipeline(\"sentiment-analysis\",model=\"Seethal/sentiment_analysis_generic_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_distilbert(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9952951073646545}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_bert_uncased(\"I hate you\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, for now, take only the lines that have been annotated by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only annotated lines\n",
    "df_filtered = annotated_ids()\n",
    "# reset index\n",
    "df_filtered = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis sentiment_analysis_roberta\n",
      "Fit sentiment analysis sentiment_analysis_bert\n",
      "Fit sentiment analysis sentiment_analysis_distilbert\n",
      "Fit sentiment analysis sentiment_analysis_bert_uncased\n",
      "sample 217 out of 217. 216.46%\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "fit_sentiment(df_filtered, sentiment_analysis_roberta, \"sentiment_analysis_roberta\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert, \"sentiment_analysis_bert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_distilbert, \"sentiment_analysis_distilbert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert_uncased, \"sentiment_analysis_bert_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis with Roberta:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6359447004608295          \n",
      "Precision: 0.6626650660264106          \n",
      "Recall: 0.7210955710955711          \n",
      "F1: 0.6188157338847753\n",
      "\n",
      "Analysis with DistilBERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6129032258064516          \n",
      "Precision: 0.636748844375963          \n",
      "Recall: 0.6861888111888111          \n",
      "F1: 0.593850267379679\n",
      "\n",
      "Analysis with BERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.5668202764976958          \n",
      "Precision: 0.5795834989383376          \n",
      "Recall: 0.633154960981048          \n",
      "F1: 0.5713713425978206\n",
      "\n",
      "Analysis with BERT Uncased:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6221198156682027          \n",
      "Precision: 0.605795265792266          \n",
      "Recall: 0.6024191750278707          \n",
      "F1: 0.602829144934408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get values of Y_val\n",
    "Y_val = df_filtered[\"Sentiment\"].values\n",
    "\n",
    "y_pred_roberta = extract_ypred(df_filtered, \"sentiment_analysis_roberta\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_roberta\")\n",
    "y_pred_bert = extract_ypred(df_filtered, \"sentiment_analysis_bert\", trans_dict_bert,  write=True, target_column=\"pred_sentiment_label_bert\")\n",
    "y_pred_bert_uncased = extract_ypred(df_filtered, \"sentiment_analysis_bert_uncased\", trans_dict_bert, write=True, target_column=\"pred_sentiment_label_bert_uncased\")\n",
    "y_pred_distilbert = extract_ypred(df_filtered, \"sentiment_analysis_distilbert\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_distilbert\")\n",
    "\n",
    "result_score(Y_val, y_pred_roberta, \"Roberta\", binary=True)\n",
    "result_score(Y_val, y_pred_distilbert, \"DistilBERT\", binary=True)\n",
    "result_score(Y_val, y_pred_bert, \"BERT\", binary=False)\n",
    "result_score(Y_val, y_pred_bert_uncased, \"BERT Uncased\", binary=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a descent accuracy, but a major limitation is that the classifier predicts into two classes (either positive or negative), while we have 3 classes (positive, neutral, negative). I solved this by setting the neutral class labeled by us to positive, but this obviously reduces the accuracy of the model by a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
