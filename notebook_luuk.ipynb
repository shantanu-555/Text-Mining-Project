{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "For the sentiment analysis, we tried out several different models and pre-processing pipelines. Especially for dealing with comments or descriptions in the lines, like [laughing] or [to camera], we tried out different methods to see which resulted in the best score for the sentiment analysis.\n",
    "\n",
    "We mainly used sentiment analysis based on pre-trained models, and then tested the accuracy by comparing the predicted sentiment with the sentiments given by us in the annotated sample (of 300 lines).\n",
    "\n",
    "## 1. Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"The_Office_lines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\"id\",\"speaker\", \"line_text\"]\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# deals with descriptions in lines, e.g. [laughs] or [to camera]\n",
    "def deal_with_description(line, mode):\n",
    "    if mode==\"remove\":\n",
    "        # remove text that is between brackets\n",
    "        line = re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"end\":\n",
    "        # move all the text that is in the brackets to the end of the line\n",
    "        line = re.sub(r'\\[.*?\\]', '', line) + \" \" + \", \".join(re.findall(r\"\\[(.*?)\\]\", line))\n",
    "    elif mode==\"start\":\n",
    "        # move all the text that is in the brackets to the start of the line\n",
    "        line = \", \".join(re.findall(r\"\\[(.*?)\\]\", line)) + \" \" + re.sub(r'\\[.*?\\]', '', line)\n",
    "    elif mode==\"keep\":\n",
    "        # remove all brackets from the line but keep text in place\n",
    "        line = re.sub(r\"[\\([{})\\]]\", '', line)\n",
    "    return line\n",
    "\n",
    "def preprocess_sentiment(df, relevant_columns, description_mode):\n",
    "    # filter out relevant columns\n",
    "    df = df[relevant_columns]\n",
    "    # deal with descriptions in lines\n",
    "    df[\"line_text\"] = df[\"line_text\"].apply(lambda x: deal_with_description(x, mode=description_mode))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = preprocess_sentiment(df, relevant_columns, description_mode=\"keep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the sentiment analysis first only the the sample labeled by us, and then applied the best performing combination of pipeline and model to the whole dataset.\n",
    "\n",
    "#### Function to extract ids that have been annotated by us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_ids():\n",
    "    df_luuk = pd.read_csv(\"annotated_data/sample_Luuk.csv\")\n",
    "    df_shan = pd.read_csv(\"annotated_data/sample_Shantanu.csv\")\n",
    "    df_elin = pd.read_csv(\"annotated_data/sample_Eline.csv\")\n",
    "\n",
    "    # combine annotations\n",
    "    df_combined = pd.concat([df_luuk, df_shan, df_elin], axis=0)\n",
    "\n",
    "    # filter out only columns that have something in \"Sentiment\" column\n",
    "    df_annotated = df_combined[df_combined[\"Sentiment\"].notna()]\n",
    "\n",
    "    return df_annotated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the accuracy of the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "# translating strings of sentiment to integers\n",
    "trans_dict_roberta = {\n",
    "    \"NEGATIVE\": -1,\n",
    "    \"POSITIVE\": 1\n",
    "}\n",
    "\n",
    "trans_dict_bert = {\n",
    "    \"LABEL_0\": -1,\n",
    "    \"LABEL_1\": 0,\n",
    "    \"LABEL_2\": 1\n",
    "}\n",
    "\n",
    "# extract predicted values from dataframe\n",
    "def extract_ypred(df, source_column, transdict, write=True, target_column=\"temp\"):\n",
    "    df[target_column] = df[source_column].apply(lambda x: transdict[x[0][\"label\"]])\n",
    "    Y_pred = df[target_column].values\n",
    "    if not write:\n",
    "        df = df.drop(columns=[target_column])\n",
    "    return Y_pred\n",
    "\n",
    "def result_score(Y_val, Y_pred, name, binary=False):\n",
    "    # make new list replacing 0 with 1 if binary\n",
    "    if binary:\n",
    "        Y_val = [1 if x==0 else x for x in Y_val]\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(Y_val, Y_pred)\n",
    "    precision = precision_score(Y_val, Y_pred, average=\"macro\")\n",
    "    recall = recall_score(Y_val, Y_pred, average=\"macro\")\n",
    "    f1 = f1_score(Y_val, Y_pred, average=\"macro\")\n",
    "    MSE = mean_squared_error(Y_val, Y_pred)\n",
    "\n",
    "    # print results\n",
    "    print(f\"Analysis with {name}:\\\n",
    "          \\n- - - - - - - - - - \\\n",
    "          \\nAccuracy: {accuracy}\\\n",
    "          \\nPrecision: {precision}\\\n",
    "          \\nRecall: {recall}\\\n",
    "          \\nF1: {f1}\\\n",
    "          \\nMSE: {MSE}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find current time\n",
    "import time\n",
    "#supress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fit_sentiment(df_filtered, method, name, progress=True):\n",
    "    # set start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # apply sentiment analysis to each line, track progress\n",
    "    df_filtered[name] = \"\"\n",
    "\n",
    "    # apply sentiment analysis to each line and track progress\n",
    "    if progress:\n",
    "        print(f\"Fit sentiment analysis {name}\")\n",
    "    k = len(df_filtered)\n",
    "    for iter, row in df_filtered.iterrows():\n",
    "        df_filtered[name][iter] = method(row[\"line_text\"])\n",
    "        if progress:\n",
    "            print(f\"sample {iter+1} out of {k}. {round((iter+1)/k*100, 2)}%  \", end='\\x1b[1K\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import first pre-trained sentiment analysis pipeline\n",
    "from transformers import pipeline\n",
    "sentiment_analysis_roberta = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "sentiment_analysis_bert  = pipeline(\"sentiment-analysis\",model=\"sbcBI/sentiment_analysis_model\")\n",
    "sentiment_analysis_distilbert = pipeline(\"sentiment-analysis\",model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_analysis_bert_uncased = pipeline(\"sentiment-analysis\",model=\"Seethal/sentiment_analysis_generic_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_distilbert(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9952951073646545}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_bert_uncased(\"I hate you\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, for now, take only the lines that have been annotated by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only annotated lines\n",
    "df_filtered = annotated_ids()\n",
    "# reset index\n",
    "df_filtered = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit sentiment analysis sentiment_analysis_roberta\n",
      "Fit sentiment analysis sentiment_analysis_bert\n",
      "Fit sentiment analysis sentiment_analysis_distilbert\n",
      "Fit sentiment analysis sentiment_analysis_bert_uncased\n",
      "sample 217 out of 217. 100.0%\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "fit_sentiment(df_filtered, sentiment_analysis_roberta, \"sentiment_analysis_roberta\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert, \"sentiment_analysis_bert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_distilbert, \"sentiment_analysis_distilbert\")\n",
    "fit_sentiment(df_filtered, sentiment_analysis_bert_uncased, \"sentiment_analysis_bert_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis with Roberta:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6359447004608295          \n",
      "Precision: 0.6626650660264106          \n",
      "Recall: 0.7210955710955711          \n",
      "F1: 0.6188157338847753          \n",
      "MSE: 1.456221198156682\n",
      "\n",
      "Analysis with DistilBERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6129032258064516          \n",
      "Precision: 0.636748844375963          \n",
      "Recall: 0.6861888111888111          \n",
      "F1: 0.593850267379679          \n",
      "MSE: 1.5483870967741935\n",
      "\n",
      "Analysis with BERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.5668202764976958          \n",
      "Precision: 0.5795834989383376          \n",
      "Recall: 0.633154960981048          \n",
      "F1: 0.5713713425978206          \n",
      "MSE: 0.5852534562211982\n",
      "\n",
      "Analysis with BERT Uncased:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.6221198156682027          \n",
      "Precision: 0.605795265792266          \n",
      "Recall: 0.6024191750278707          \n",
      "F1: 0.602829144934408          \n",
      "MSE: 0.48847926267281105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get values of Y_val\n",
    "Y_val = df_filtered[\"Sentiment\"].values\n",
    "\n",
    "y_pred_roberta = extract_ypred(df_filtered, \"sentiment_analysis_roberta\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_roberta\")\n",
    "y_pred_bert = extract_ypred(df_filtered, \"sentiment_analysis_bert\", trans_dict_bert,  write=True, target_column=\"pred_sentiment_label_bert\")\n",
    "y_pred_bert_uncased = extract_ypred(df_filtered, \"sentiment_analysis_bert_uncased\", trans_dict_bert, write=True, target_column=\"pred_sentiment_label_bert_uncased\")\n",
    "y_pred_distilbert = extract_ypred(df_filtered, \"sentiment_analysis_distilbert\", trans_dict_roberta, write=True, target_column=\"pred_sentiment_label_distilbert\")\n",
    "\n",
    "result_score(Y_val, y_pred_roberta, \"Roberta\", binary=True)\n",
    "result_score(Y_val, y_pred_distilbert, \"DistilBERT\", binary=True)\n",
    "result_score(Y_val, y_pred_bert, \"BERT\", binary=False)\n",
    "result_score(Y_val, y_pred_bert_uncased, \"BERT Uncased\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_analysis_roberta</th>\n",
       "      <th>sentiment_analysis_bert</th>\n",
       "      <th>sentiment_analysis_distilbert</th>\n",
       "      <th>sentiment_analysis_bert_uncased</th>\n",
       "      <th>pred_sentiment_label_roberta</th>\n",
       "      <th>pred_sentiment_label_bert</th>\n",
       "      <th>pred_sentiment_label_bert_uncased</th>\n",
       "      <th>pred_sentiment_label_distilbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47252</td>\n",
       "      <td>Gabe</td>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.983961701393...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.7348992228507...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.768441677093...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9918521046638...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15710</td>\n",
       "      <td>Andy</td>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997746646404...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.4067431092262...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.993637084960...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9106979966163...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44150</td>\n",
       "      <td>Dwight</td>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998121678829...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.3897318840026...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.903421878814...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.5951375961303...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45628</td>\n",
       "      <td>Phyllis</td>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.993801534175...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.4273286163806...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998319804668...</td>\n",
       "      <td>[{'label': 'LABEL_1', 'score': 0.9912397265434...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27785</td>\n",
       "      <td>Pam</td>\n",
       "      <td>Oh, damn. [Pam looks down at her salad] They'v...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.999482750892...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.6407026648521...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.994488358497...</td>\n",
       "      <td>[{'label': 'LABEL_0', 'score': 0.9335484504699...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  speaker                                          line_text  \\\n",
       "0  47252     Gabe  Ok, but once this starts, it's going to be mov...   \n",
       "1  15710     Andy                                              What?   \n",
       "2  44150   Dwight  Just a little announcement folks, remember, th...   \n",
       "3  45628  Phyllis  Is it true that you're making Dwight the manager?   \n",
       "4  27785      Pam  Oh, damn. [Pam looks down at her salad] They'v...   \n",
       "\n",
       "   Sentiment                         sentiment_analysis_roberta  \\\n",
       "0       -1.0  [{'label': 'POSITIVE', 'score': 0.983961701393...   \n",
       "1        0.0  [{'label': 'NEGATIVE', 'score': 0.997746646404...   \n",
       "2        1.0  [{'label': 'POSITIVE', 'score': 0.998121678829...   \n",
       "3       -1.0  [{'label': 'NEGATIVE', 'score': 0.993801534175...   \n",
       "4        0.0  [{'label': 'NEGATIVE', 'score': 0.999482750892...   \n",
       "\n",
       "                             sentiment_analysis_bert  \\\n",
       "0  [{'label': 'LABEL_0', 'score': 0.7348992228507...   \n",
       "1  [{'label': 'LABEL_0', 'score': 0.4067431092262...   \n",
       "2  [{'label': 'LABEL_0', 'score': 0.3897318840026...   \n",
       "3  [{'label': 'LABEL_1', 'score': 0.4273286163806...   \n",
       "4  [{'label': 'LABEL_0', 'score': 0.6407026648521...   \n",
       "\n",
       "                       sentiment_analysis_distilbert  \\\n",
       "0  [{'label': 'NEGATIVE', 'score': 0.768441677093...   \n",
       "1  [{'label': 'NEGATIVE', 'score': 0.993637084960...   \n",
       "2  [{'label': 'NEGATIVE', 'score': 0.903421878814...   \n",
       "3  [{'label': 'POSITIVE', 'score': 0.998319804668...   \n",
       "4  [{'label': 'NEGATIVE', 'score': 0.994488358497...   \n",
       "\n",
       "                     sentiment_analysis_bert_uncased  \\\n",
       "0  [{'label': 'LABEL_1', 'score': 0.9918521046638...   \n",
       "1  [{'label': 'LABEL_1', 'score': 0.9106979966163...   \n",
       "2  [{'label': 'LABEL_0', 'score': 0.5951375961303...   \n",
       "3  [{'label': 'LABEL_1', 'score': 0.9912397265434...   \n",
       "4  [{'label': 'LABEL_0', 'score': 0.9335484504699...   \n",
       "\n",
       "   pred_sentiment_label_roberta  pred_sentiment_label_bert  \\\n",
       "0                             1                         -1   \n",
       "1                            -1                         -1   \n",
       "2                             1                         -1   \n",
       "3                            -1                          0   \n",
       "4                            -1                         -1   \n",
       "\n",
       "   pred_sentiment_label_bert_uncased  pred_sentiment_label_distilbert  \n",
       "0                                  0                               -1  \n",
       "1                                  0                               -1  \n",
       "2                                 -1                               -1  \n",
       "3                                  0                                1  \n",
       "4                                 -1                               -1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a descent accuracy, but a major limitation is that the classifier predicts into two classes (either positive or negative), while we have 3 classes (positive, neutral, negative). I solved this by setting the neutral class labeled by us to positive, but this obviously reduces the accuracy of the model by a lot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_filtered[[\"line_text\", \"Sentiment\", \"pred_sentiment_label_roberta\", \"pred_sentiment_label_bert\", \"pred_sentiment_label_bert_uncased\", \"pred_sentiment_label_distilbert\"]]\n",
    "\n",
    "# rename columns\n",
    "df_compare.columns = [\"line_text\", \"Annotated\", \"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to combine measures\n",
    "\n",
    "1. Voting: take the majority vote of the different sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis with Majority:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.4792626728110599          \n",
      "Precision: 0.5413793103448276          \n",
      "Recall: 0.6029208472686733          \n",
      "F1: 0.48141718106995884          \n",
      "MSE: 0.6728110599078341\n",
      "\n",
      "Analysis with Majority_minus_BERT:          \n",
      "- - - - - - - - - -           \n",
      "Accuracy: 0.39631336405529954          \n",
      "Precision: 0.2768552311435523          \n",
      "Recall: 0.5612820512820513          \n",
      "F1: 0.3657848324514991          \n",
      "MSE: 0.8248847926267281\n",
      "\n",
      "MSE for Average:      \n",
      "- - - - - - - - - -      \n",
      "0.38623271889400923\n",
      "\n",
      "MSE for Average minus BERT:      \n",
      "- - - - - - - - - -      \n",
      "0.43317972350230416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/anaconda3/envs/textmining/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# make a new column with the majority vote\n",
    "df_compare[\"Majority\"] = df_compare[[\"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]].mode(axis=1)[0]\n",
    "df_compare[\"Majority_minus_BERT\"] = df_compare[[\"Roberta\", \"Bert_uncased\", \"Distilbert\"]].mode(axis=1)[0]\n",
    "\n",
    "# make a new column with the average vote\n",
    "df_compare[\"Average\"] = df_compare[[\"Roberta\", \"Bert\", \"Bert_uncased\", \"Distilbert\"]].mean(axis=1)\n",
    "df_compare[\"Average_minus_BERT\"] = df_compare[[\"Roberta\", \"Bert_uncased\", \"Distilbert\"]].mean(axis=1)\n",
    "\n",
    "Y_pred_majority = df_compare[\"Majority\"].values\n",
    "Y_pred_majority_minus = df_compare[\"Majority_minus_BERT\"].values\n",
    "Y_pred_average = df_compare[\"Average\"].values\n",
    "Y_pred_average_minus = df_compare[\"Average_minus_BERT\"].values\n",
    "\n",
    "result_score(Y_val, Y_pred_majority, \"Majority\", binary=False)\n",
    "result_score(Y_val, Y_pred_majority_minus, \"Majority_minus_BERT\", binary=False)\n",
    "\n",
    "print(f\"MSE for Average:\\\n",
    "      \\n- - - - - - - - - -\\\n",
    "      \\n{mean_squared_error(Y_val, Y_pred_average)}\\n\")\n",
    "\n",
    "print(f\"MSE for Average minus BERT:\\\n",
    "      \\n- - - - - - - - - -\\\n",
    "      \\n{mean_squared_error(Y_val, Y_pred_average_minus)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_text</th>\n",
       "      <th>Annotated</th>\n",
       "      <th>Roberta</th>\n",
       "      <th>Bert</th>\n",
       "      <th>Bert_uncased</th>\n",
       "      <th>Distilbert</th>\n",
       "      <th>Majority</th>\n",
       "      <th>Majority_minus_BERT</th>\n",
       "      <th>Average</th>\n",
       "      <th>Average_minus_BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok, but once this starts, it's going to be mov...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just a little announcement folks, remember, th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is it true that you're making Dwight the manager?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, damn. [Pam looks down at her salad] They'v...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           line_text  Annotated  Roberta  \\\n",
       "0  Ok, but once this starts, it's going to be mov...       -1.0        1   \n",
       "1                                              What?        0.0       -1   \n",
       "2  Just a little announcement folks, remember, th...        1.0        1   \n",
       "3  Is it true that you're making Dwight the manager?       -1.0       -1   \n",
       "4  Oh, damn. [Pam looks down at her salad] They'v...        0.0       -1   \n",
       "\n",
       "   Bert  Bert_uncased  Distilbert  Majority  Majority_minus_BERT  Average  \\\n",
       "0    -1             0          -1      -1.0                 -1.0    -0.25   \n",
       "1    -1             0          -1      -1.0                 -1.0    -0.75   \n",
       "2    -1            -1          -1      -1.0                 -1.0    -0.50   \n",
       "3     0             0           1       0.0                 -1.0     0.00   \n",
       "4    -1            -1          -1      -1.0                 -1.0    -1.00   \n",
       "\n",
       "   Average_minus_BERT  \n",
       "0            0.000000  \n",
       "1           -0.666667  \n",
       "2           -0.333333  \n",
       "3            0.000000  \n",
       "4           -1.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
